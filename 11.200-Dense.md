https://keras.io/layers/core/





```python
keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
```

Just your regular densely-connected NN layer.

`Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel`is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`).

Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`.



* **Example**

```python
# as first layer in a sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# now the model will take as input arrays of shape (*, 16)
# and output arrays of shape (*, 32)

# after the first layer, you don't need to specify
# the size of the input anymore:
model.add(Dense(32))
```

* **Arguments**

```python
units: 输出空间的维度
activation: 使用的激活函数，如果没有指定，默认为 (ie. "linear" activation: a(x) = x).
use_bias: 是否使用偏置
kernel_initializer: 初始化权值
bias_initializer: 初始化偏置
kernel_regularizer: 核正则化
bias_regularizer: 偏置正则化
activity_regularizer: 输出层的激活函数的正则化
kernel_constraint: 核权值矩阵的限制
bias_constraint: Constraint function applied to the bias vector (see constraints).
```

* **Input shape**

nD tensor with shape: `(batch_size, ..., input_dim)`. The most common situation would be a 2D input with shape `(batch_size, input_dim)`.